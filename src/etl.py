# -*- coding: utf-8 -*-
"""copyCSV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EboFi9CB6cmWR2QegB_skt8dp9MXTuug
"""

!pip -q install pandas psycopg2-binary SQLAlchemy python-dotenv
from google.colab import drive
drive.mount('/content/drive')

# Ajuste o caminho do seu CSV no Google Drive
RAW_PATH = '/content/drive/MyDrive/arquivo_limpo.csv'

import os
import psycopg2
from io import StringIO
import pandas as pd

# --- Dados fixos da conexão Neon ---
PGHOST     = "ep-still-dew-ac2tkx1l-pooler.sa-east-1.aws.neon.tech"
PGPORT     = "5432"
PGDATABASE = "neondb"
PGUSER     = "neondb_owner"
PGPASSWORD = "npg_UMJh7Wbo2fad"

# --- Tabelas ---
STAGING_TABLE = "stg_vendas"
FINAL_TABLE   = "vendas"

# Mapeamento de colunas
COLMAP = {
    "Transaction ID": "transaction_id",
    "Customer ID": "customer_id",
    "Gender": "gender",
    "Age": "age",
    "Product Category": "product_category",
    "Quantity": "quantity",
    "Price per Unit": "price_per_unit",
    "Total Amount": "total_amount",
    "Month": "month",
    "Month_Num": "month_num",
    "AnoMes": "anomes",
    "Faixa_Etaria": "faixa_etaria",
}

# Tipos de dados esperados
DTYPES = {
    "transaction_id": "Int64",
    "customer_id": "string",
    "gender": "string",
    "age": "Int64",
    "product_category": "string",
    "quantity": "Int64",
    "price_per_unit": "float",
    "total_amount": "float",
    "month": "string",
    "month_num": "Int64",
    "anomes": "string",
    "faixa_etaria": "string",
}

def get_conn():
    return psycopg2.connect(
        host=PGHOST, port=PGPORT, dbname=PGDATABASE, user=PGUSER, password=PGPASSWORD, sslmode="require"
    )

def read_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    df = df.rename(columns=COLMAP)
    return df

def validate_df(df: pd.DataFrame):
    expected = list(DTYPES.keys())
    miss = [c for c in expected if c not in df.columns]
    if miss:
        raise ValueError(f"Colunas faltando no CSV: {miss}")

    df = df[expected].copy()

    good_rows = []
    bad_rows = []
    for _, row in df.iterrows():
        try:
            r = {}
            for col, dtype in DTYPES.items():
                v = row[col]
                if dtype == "Int64":
                    r[col] = pd.to_numeric(v, errors="raise", downcast="integer")
                elif dtype == "float":
                    r[col] = float(v) if pd.notna(v) else None
                else:
                    r[col] = None if pd.isna(v) else str(v)
            good_rows.append(r)
        except Exception as e:
            rr = row.to_dict()
            rr["_error"] = str(e)
            bad_rows.append(rr)

    df_ok  = pd.DataFrame(good_rows, columns=expected).astype(DTYPES) if good_rows else pd.DataFrame(columns=expected)
    df_bad = pd.DataFrame(bad_rows) if bad_rows else pd.DataFrame(columns=expected+["_error"])

    if not df_bad.empty:
        os.makedirs("rejects", exist_ok=True)
        df_bad.to_csv("rejects/vendas_bad_rows.csv", index=False)
    return df_ok, df_bad

def ensure_staging(conn):
    ddl = f"""
    CREATE TABLE IF NOT EXISTS {STAGING_TABLE} (
        transaction_id INTEGER,
        customer_id VARCHAR(20),
        gender VARCHAR(10),
        age INTEGER,
        product_category VARCHAR(50),
        quantity INTEGER,
        price_per_unit NUMERIC(10,2),
        total_amount NUMERIC(10,2),
        month VARCHAR(15),
        month_num INTEGER,
        anomes VARCHAR(7),
        faixa_etaria VARCHAR(20)
    );
    TRUNCATE {STAGING_TABLE};
    """
    with conn.cursor() as cur:
        cur.execute(ddl)
    conn.commit()

def copy_to_staging(conn, df: pd.DataFrame):
    buf = StringIO()
    df.to_csv(buf, index=False, header=False)
    buf.seek(0)
    copy_sql = f"""
    COPY {STAGING_TABLE} (
        transaction_id, customer_id, gender, age, product_category, quantity,
        price_per_unit, total_amount, month, month_num, anomes, faixa_etaria
    )
    FROM STDIN WITH CSV
    """
    with conn.cursor() as cur:
        cur.copy_expert(copy_sql, buf)
    conn.commit()

def upsert_final(conn):
    sql = f"""
    CREATE TABLE IF NOT EXISTS {FINAL_TABLE} (
        transaction_id INTEGER PRIMARY KEY,
        customer_id VARCHAR(20),
        gender VARCHAR(10),
        age INTEGER,
        product_category VARCHAR(50),
        quantity INTEGER,
        price_per_unit NUMERIC(10,2),
        total_amount NUMERIC(10,2),
        month VARCHAR(15),
        month_num INTEGER,
        anomes VARCHAR(7),
        faixa_etaria VARCHAR(20)
    );

    INSERT INTO {FINAL_TABLE} AS tgt (
        transaction_id, customer_id, gender, age, product_category, quantity,
        price_per_unit, total_amount, month, month_num, anomes, faixa_etaria
    )
    SELECT
        transaction_id, customer_id, gender, age, product_category, quantity,
        price_per_unit, total_amount, month, month_num, anomes, faixa_etaria
    FROM {STAGING_TABLE}
    ON CONFLICT (transaction_id) DO UPDATE SET
        customer_id    = EXCLUDED.customer_id,
        gender         = EXCLUDED.gender,
        age            = EXCLUDED.age,
        product_category = EXCLUDED.product_category,
        quantity       = EXCLUDED.quantity,
        price_per_unit = EXCLUDED.price_per_unit,
        total_amount   = EXCLUDED.total_amount,
        month          = EXCLUDED.month,
        month_num      = EXCLUDED.month_num,
        anomes         = EXCLUDED.anomes,
        faixa_etaria   = EXCLUDED.faixa_etaria;
    """
    with conn.cursor() as cur:
        cur.execute(sql)
    conn.commit()

# --- EXECUÇÃO ---
print("Lendo CSV:", RAW_PATH)
df_raw = read_csv(RAW_PATH)
df_ok, df_bad = validate_df(df_raw)
print(f"Linhas válidas: {len(df_ok)} | Rejeitadas: {len(df_bad)} (arquivo salvo em ./rejects se houver)")

with get_conn() as conn:
    ensure_staging(conn)
    print("Carregando para STAGING via COPY…")
    copy_to_staging(conn, df_ok)
    print("UPSERT para tabela final…")
    upsert_final(conn)

print("ETL concluído!")